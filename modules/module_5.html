
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 4 - NLP Course DMGK</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #8B0000;
        }
        .notebook-link {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            margin-top: 10px;
        }
        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            z-index: 1000;
        }
        .citation {
            margin-top: 10px;
            padding-left: 20px;
            border-left: 3px solid #8B0000;
        }
        .task {
            margin-bottom: 15px;
        }
    </style>
</head>
<body>
    <a href="../index.html" class="btn btn-secondary back-button">&larr; Go Back</a>

    <div class="container">
        <h1>Module 5: Large Language Models for Article Extraction and Post-OCR Correction</h1>
        
        <p>Module 5 will be all about Large Language models, prompting techniques and two specific NLP taks: article extraction and OCR post-correction</p>
        <ul>
           Large Language Models (LLMs) are artificial intelligence systems trained on massive text datasets that can process and generate human language based on the Transformer architecture introduced by Vaswear et al. in 2017. These models use neural networks to predict likely next tokens in a sequence, enabling tasks like text completion, translation, and question answering. While research shows correlations between model size, training data, and performance, specific capabilities and limitations continue to be actively studied and debated in the research community. They fundamentally operate through pattern matching rather than genuine understanding.
        </ul>
        
      <h3>Preparation for Module 5:</h3>
<ol>
    <li>Watch (if not done already) this YouTube Video on LLMs: <a href="https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5">3Blue1Brown: Large Language Models</a></li>
    <li>Check out the Preprint listed under Literature. Did you know about those prompting techniques?</li>
    <li>Create an NVIDIA token:
        <ol>
            <li>Visit the <a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct">NVIDIA AI Playground</a></li>
            <li>Click on login</li>
            <li>Enter your University Email</li>
            <li>Copy the token</li>
        </ol>
    </li>
</ol>
        <h3>Literature:</h3> 
                   <p class="citation">
    Sahoo, P., Singh, A. K., Saha, S., Jain, V., Mondal, S., & Chadha, A. A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications. arXiv:2402.07927 (2024). <a href="https://doi.org/10.48550/arXiv.2402.07927" target="_blank">https://doi.org/10.48550/arXiv.2402.07927</a>
                   </p>    

        
        <h3>Notebooks we will use in class:</h3>
    <div class="notebook-link">
             <p>Large Language Models and Article_Separation</p>
                <a href="https://colab.research.google.com/github/ieg-dhr/NLP-Course4Humanities_2024/blob/main/Large_Language_Models_Article_Separation.ipynb" target="_blank">
                    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Large_Language_Models_Article_Separation.ipynb In Colab"/>
        
                </a>
            </div>
    <div class="notebook-link">
        <p>Prompting Techniques and LLMs</p>

        
        <h3>Workload (after class):</h3>
    
                <p>Write a prompt for your own topic and try the notebooks with your own data:</p>

        
        <h3>Date and Time:</h3>
        <p>December 6, 2024 (10:00 AM to 11:30 AM)</p>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
